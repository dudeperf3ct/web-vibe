## Developer Logs

Idea: Create a project that helps content writers and graphic designers evaluate their website with help of LLMs.

The theme of the project is to learn about LLM Agents.

(13:42) Let's go :runner:

(13:43) Hmm, what should I name this project? (*Switches the tab and asks GPT-4o to come up with something cool*). Ok, Web-Vibe it is! Create a github repo.

(13:44) I wanted to test [uv tool](https://docs.astral.sh/uv/) for creating the project. 

**Question**: Is it good idea to go with untested tools for first time in hackathon, probably might takes more time to learn about tool rather than work on the project.

(13:54) Let's dive into understanding LLM Agents first. There's a great 5-part series by Andrew Ng on Agentic Design Patterns.

### Agentic Design Patterns Part 1

Link: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/

LLMs are stateless generating a token by token. Using LLM, we consider the final generated text as our output. In most cases, getting the right output in first attempt might work (zero-shot prompting) depending on the task. In some cases, multiple revisions are required to accomplish the same task. 

I found the analogy of writing an essay in the first attempt described in the post helpful and how agentic workflow works. 

> This is akin to asking someone to compose an essay from start to finish, typing straight through with no backspacing allowed, and expecting a  high-quality result.

The first draft generated by LLMs will takes us maybe 80% of the way. It will require multiple iterations to complete it similar to how we would do the task.

> With an agent workflow, however, we can ask the LLM to iterate over a document many times. For example, it might take a sequence of steps  such as:
>
> - Plan an outline.
> - Decide what, if any, web searches are needed to gather more information.
> - Write a first draft.
> - Read over the first draft to spot unjustified arguments or extraneous information.
> - Revise the draft taking into account any weaknesses spotted.
> - And so on.

There are 4 design patterns for building LLM agents

> - Reflection: The LLM examines its own work to come up with ways to improve it. 
> - Tool Use: The LLM is given tools such as web search, code execution, or any  other function to help it gather information, take action, or process  data.
> - Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay,  then doing online research, then writing a draft, and so on).
> - Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions  than a single agent would.

Let's quickly go through each to understand what an LLM agent is composed of.

### Agentic Design Patterns Part 2, Reflection

Link: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/

In this pattern, we ask LLM to reflect on the generated output -- critiquing -- to get an improved response based on the feedback.

> Take the task of asking an LLM to write code. We can prompt it to  generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:
>
> Here’s code intended for task X: [previously generated code]  
> Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.

TL;DR: Reflection pattern consists of two agents: one generating output and other critiquing the first agent's output. The combined back and forth amongst two agents will lead to improved response. 

(*How it might relate to the project*) We are asking LLM to critique based on the screenshot of the page. Here, we expect someone to provide an screenshot to the LLM. The feedback from LLM can be used by content writer or graphic designer to improve the current iteration.

### Agentic Design Patterns Part 3, Tool Use

Link: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use

In this pattern, we use an external tool to help reduce the hallucination (--feature--) of an LLM. The tool can provide necessary grounding or context for LLM to answer the query. 

Delegating math tasks to a tool seems like a good approach rather than relying on LLMs math skill. 

> Developers are using functions to search different sources (web,  Wikipedia, arXiv, etc.), to interface with productivity tools (send  email, read/write calendar entries, etc.), generate or interpret images, and much more. We can prompt an LLM using context that gives detailed  descriptions of many functions. These descriptions might include a text  description of what the function does plus details of what arguments the function expects. <ins>And we’d expect the LLM to automatically choose the right function to call to do a job.</ins> Further, systems are being built in which the LLM has access to hundreds of tools. In such settings, there might be too many functions at your disposal to put all of them into the LLM context, so you might use heuristics to pick the most relevant  subset to include in the LLM context at the current step of processing. 

Rather than us choosing which tool to use, we can ask LLM to choose a tool as well. That's cool. 

(*How it might relate to the project*) We are asking LLM to critique based on the screenshot of the page. Here, we expect someone to provide an screenshot to the LLM. Hmm, looks like a good way to use a tool that could accomplish this task for us. We can ask LLM to use that tool to create a screenshot and pass it to LLM.

### Agentic Design Patterns Part 4, Planning

Link: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning

In this pattern, we ask LLM to come up with a plan before starting the task. Breaking down the task into small steps and let LLM figure out steps on how to achieve sounds like how we would approach the task as well.

> Many tasks can’t be done in a single step or with a single tool  invocation, but an agent can decide what steps to take. For example, to  simplify an example from the HuggingGPT paper (cited below), if you want an agent to consider a picture of a boy and draw a picture of a girl in the same pose, the task might be decomposed into two distinct steps:  (i) detect the pose in the picture of the boy and (ii) render a picture  of a girl in the detected pose. An LLM might be fine-tuned or prompted  (with few-shot prompting) to specify a plan by outputting a string like *"{tool: pose-detection, input: image.jpg, output: temp1 } {tool: pose-to-image, input: temp1, output: final.jpg}"*. 
>
> Admittedly, many agentic workflows do not need planning. For example,  you might have an agent reflect on, and improve, its output a fixed  number of times. In this case, the sequence of steps the agent takes is  fixed and deterministic. But for complex tasks in which you aren’t able  to specify a decomposition of the task into a set of steps ahead of  time, Planning allows the agent to decide dynamically what steps to  take. 

(*How it might relate to the project*) In the first iteration, I am not sure. But more advanced workflow for this project would be to self generate images based on **reflection** and using **tool** design patterns. A more more advance workflow would generate the code itself. Cheeky, right? Last step, AGI :stuck_out_tongue:.​

### Agentic Design Patterns Part 5, Multi-Agent Collaboration

Link: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration

Multi-agent sounds like let LLM agents cook and we just enjoy the meal 😋🍽️ .

> Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles —  such as a software engineer, product manager, designer, QA (quality  assurance) engineer, and so on — and have different agents accomplish different subtasks

(*How it might relate to the project*) Multi-agent workflow would be similar to one described in planning above. The advanced workflow could delegate the task of creating new images or content and further reflecting on those generated output.

---

(14:43) Have some idea on what LLM agents are! Let's get cooking 🧑‍🍳

> Agentic workflow is basically each agent specialising in a task and we have this grand orchestrator (another agent?) to manage all these agents. 

Reflecting back on my project, for the first iteration, it might not look like we will be hitting all these LLM agentic design patterns describe above.

The plan for the workflow (sorry LLM planning agent, but let me take that task from you!)

- How to grab a screenshot given a URL of the website?
- Pass the screenshot to an LLM and let it critique based on the theme of the website.
- Do we need to also provide a summary of what webpage or theme of the website is? For this, we will need to get the content of the site and pass it to an LLM for summarization task. (Let's circle back depending on how output looks like after implementing two steps above.)

(*Developer sidebar*) The first iteration does not look like any LLM agent workflow. The more advanced workflows described in *(How it might relate to project)* sections can be extended to be complete agentic workflow

> The advanced workflow could delegate the task of creating new images or content and further reflecting on those generated output. A more more advance workflow would generate the code itself to self-improve the current iteration.

---

(14:50) Based on the first iteration plan, how can we tackle the first point.

> How to grab a screenshot given a URL of the website?

Looks like there are couple of ways we can do this,

- Use Selenium to take screenshot. [Tutorial](https://pythonbasics.org/selenium-screenshot/)
- Use Playwright to grab screenshots. [Playwright Docs](https://playwright.dev/python/docs/screenshots)

There are few projects, LLM agents that provide enhanced functionalities

- Crawl4AI : https://github.com/unclecode/crawl4ai
- Crawlee : https://crawlee.dev/python/

I quite like Crawlee. They have a nice wrapper around Playwright : `PlaywrightCrawler` that does all of the heavy lifting.

Docs: https://crawlee.dev/python/docs/examples/playwright-crawler

(15:56) Looks like we have first iteration that grabs screenshot of the website.

**Limitation**: Only the homepage of the website is screenshot and stored.

---

(15:58) Tackling the second and third point

> Pass the screenshot to an LLM and let it critique based on the theme of the website.
>
> Do we need to also provide a summary of what webpage or theme of the website is? For this, we will need to get the content of the site and pass it to an LLM for summarization task. (Let's circle back depending on how output looks like after implementing two steps above.)

There are so many LLM gateway libraries to pick from and too many LLMs to choose from.

The initial iteration using ChatGPT looks fine. There might be need to perform some prompt engineering on how the output is returned.

I wiill use [litellm](https://github.com/BerriAI/litellm) library that I had used in past projects.

The first iteration is complete. Next, I will add a streamlit front-end to make it easy to interact with this.

Streamlit does not play well with asyncio and it's functions. Switching to gradio instead. Worked only for 1 url failed for multiple submit with same error.

(18:48) Instead of building front end let's use it to visualize the results.

---

(20:27) Back from dinner. First pass looks okay, shame the front end is not working as expected. Let's build a first reflection agent that helps content writer to improve the language or tone of the content.

> Do we need to also provide a summary of what webpage or theme of the website is? For this, we will need to get the content of the site and pass it to an LLM for summarization task. (Let's circle back depending on how output looks like after implementing two steps above.)

This was not required from the first pass. My guess is adding this as part of the prompt along with screenshot could improve the analysis for LLM. (TO BE TESTED) 
